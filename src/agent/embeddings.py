"""Embedding utilities and document processing."""
from typing import List, Dict, Any
import os
from langchain_text_splitters import SentenceTransformersTokenTextSplitter
from langchain_community.document_loaders import WebBaseLoader, TextLoader, DirectoryLoader
from langchain_community.document_loaders.web_base import WebBaseLoader
from pathlib import Path
import requests
from bs4.filter import SoupStrainer
from bs4 import BeautifulSoup


class DocumentProcessor:
    """Process and prepare documents for embedding."""
    
    def __init__(self, config):
        self.config = config
        
        # Use SentenceTransformersTokenTextSplitter for better token management
        self.text_splitter = SentenceTransformersTokenTextSplitter(
            model_name=config.embedding_model,
            tokens_per_chunk=256,  # Optimal for most sentence transformer models
            chunk_overlap=50,
            # separators=["\n\n", "\n", ". ", " "]
        )
    
    def load_documents_from_urls(self, urls: List[str]) -> List[Dict[str, Any]]:
        """Load and process documents from web URLs."""
        if not urls:
            print("No URLs provided.")
            return []
        
        processed_docs = []
        
        for url in urls:
            try:
                print(f"Loading from URL: {url}")
                
                # Create WebBaseLoader for the URL
                loader = WebBaseLoader(
                    web_paths=[url],
                    # bs_kwargs={
                    #     "parse_only": SoupStrainer(
                    #         ["article", "main", "div", "p", "h1", "h2", "h3", "h4", "h5", "h6"]
                    #     )
                    # }
                )
                
                # Load documents
                docs = loader.load()
                
                if not docs:
                    print(f"No content found at {url}")
                    continue
                
                # Process each document
                for doc in docs:
                    # Clean and extract text content
                    content = doc.page_content.strip()
                    if len(content) < 100:  # Skip very short content
                        continue
                    
                    # Split into chunks
                    chunks = self.text_splitter.split_text(content)
                    
                    # Create document entries for each chunk
                    for i, chunk in enumerate(chunks):
                        if len(chunk.strip()) > 50:  # Only keep meaningful chunks
                            processed_docs.append({
                                "content": chunk.strip(),
                                "source": url,
                                "title": self._extract_title_from_url(url),
                                "chunk_id": i,
                                "total_chunks": len(chunks),
                                "url": url
                            })
                
                print(f"‚úÖ Processed {len([d for d in processed_docs if d['source'] == url])} chunks from {url}")
                
            except Exception as e:
                print(f"‚ùå Error loading {url}: {str(e)}")
                continue
        
        print(f"üìö Total processed: {len(processed_docs)} document chunks from {len(urls)} URLs")
        return processed_docs
    
    def load_documents_from_directory(self, directory_path: str) -> List[Dict[str, Any]]:
        """Load and process documents from a directory (fallback method)."""
        if not os.path.exists(directory_path):
            print(f"Directory {directory_path} does not exist.")
            return []
        
        # Load documents
        loader = DirectoryLoader(
            directory_path,
            glob="**/*.txt",
            loader_cls=TextLoader,
            loader_kwargs={'encoding': 'utf-8'}
        )
        
        try:
            raw_docs = loader.load()
            print(f"Loaded {len(raw_docs)} raw documents")
        except Exception as e:
            print(f"Error loading documents: {e}")
            return []
        
        # Split documents into chunks using SentenceTransformers splitter
        processed_docs = []
        for doc in raw_docs:
            chunks = self.text_splitter.split_text(doc.page_content)
            
            for i, chunk in enumerate(chunks):
                processed_docs.append({
                    "content": chunk,
                    "source": doc.metadata.get("source", "unknown"),
                    "title": Path(doc.metadata.get("source", "")).stem,
                    "chunk_id": i,
                    "total_chunks": len(chunks)
                })
        
        print(f"Created {len(processed_docs)} document chunks")
        return processed_docs

    def process_text_input(self, text: str, source: str = "user_input") -> List[Dict[str, Any]]:
        """Process a single text input into chunks."""
        chunks = self.text_splitter.split_text(text)
        
        processed_docs = []
        for i, chunk in enumerate(chunks):
            processed_docs.append({
                "content": chunk,
                "source": source,
                "title": "User Input",
                "chunk_id": i,
                "total_chunks": len(chunks)
            })
        
        return processed_docs
    
    def _extract_title_from_url(self, url: str) -> str:
        """Extract a meaningful title from URL."""
        try:
            # Try to get title from the page
            response = requests.get(url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})
            soup = BeautifulSoup(response.text, 'html.parser')
            
            title_tag = soup.find('title')
            if title_tag and title_tag.text.strip():
                return title_tag.text.strip()[:100]  # Limit title length
            
            # Fallback: use URL path
            from urllib.parse import urlparse
            parsed = urlparse(url)
            path_parts = [part for part in parsed.path.split('/') if part]
            if path_parts:
                return path_parts[-1].replace('-', ' ').replace('_', ' ').title()
            
            return parsed.netloc
            
        except Exception:
            # Final fallback: use domain name
            from urllib.parse import urlparse
            return urlparse(url).netloc


def setup_web_documents() -> List[str]:
    """Return a list of sample web URLs for testing."""
    sample_urls = [
        # Vietnamese content
        "https://vi.wikipedia.org/wiki/H%E1%BB%93_Ch%C3%AD_Minh",
        "https://vi.wikipedia.org/wiki/L%E1%BB%8Bch_s%E1%BB%AD_Vi%E1%BB%87t_Nam",
        
        # Technology content  
        "https://python.langchain.com/docs/introduction/",
        "https://python.langchain.com/docs/tutorials/rag/",
        
        # AI/ML content
        "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "https://en.wikipedia.org/wiki/Large_language_model"
    ]
    
    print(f"üìã Sample URLs for document loading:")
    for i, url in enumerate(sample_urls, 1):
        print(f"  {i}. {url}")
    
    return sample_urls


def setup_sample_documents(data_dir: str = "./data/documents") -> List[str]:
    """Create sample documents for testing (fallback if web loading fails)."""
    os.makedirs(data_dir, exist_ok=True)
    
    sample_docs = {
        "vietnam_history.txt": """
L·ªãch s·ª≠ Vi·ªát Nam l√† m·ªôt ch·∫∑ng ƒë∆∞·ªùng d√†i v·ªõi nhi·ªÅu bi·∫øn ƒë·ªông. 
Vi·ªát Nam c√≥ l·ªãch s·ª≠ h√†ng ng√†n nƒÉm v·ªõi nhi·ªÅu tri·ªÅu ƒë·∫°i phong ki·∫øn.
Trong th·∫ø k·ª∑ 20, Vi·ªát Nam ƒë√£ tr·∫£i qua c√°c cu·ªôc kh√°ng chi·∫øn ch·ªëng th·ª±c d√¢n Ph√°p v√† ƒë·∫ø qu·ªëc M·ªπ.
H·ªì Ch√≠ Minh l√† l√£nh t·ª• vƒ© ƒë·∫°i c·ªßa d√¢n t·ªôc Vi·ªát Nam, ng∆∞·ªùi ƒë√£ d·∫´n d·∫Øt cu·ªôc ƒë·∫•u tranh gi√†nh ƒë·ªôc l·∫≠p.
Ng√†y 2/9/1945 l√† ng√†y Vi·ªát Nam tuy√™n b·ªë ƒë·ªôc l·∫≠p, th√†nh l·∫≠p n∆∞·ªõc Vi·ªát Nam D√¢n ch·ªß C·ªông h√≤a.
Chi·∫øn tranh Vi·ªát Nam k√©o d√†i t·ª´ 1955 ƒë·∫øn 1975, k·∫øt th√∫c v·ªõi vi·ªác th·ªëng nh·∫•t ƒë·∫•t n∆∞·ªõc.
Sau 1975, Vi·ªát Nam b∆∞·ªõc v√†o th·ªùi k·ª≥ ƒë·ªïi m·ªõi t·ª´ nƒÉm 1986, m·ªü c·ª≠a kinh t·∫ø.
""",
        
        "technology.txt": """
Tr√≠ tu·ªá nh√¢n t·∫°o (AI) ƒëang ph√°t tri·ªÉn m·∫°nh m·∫Ω v√† thay ƒë·ªïi nhi·ªÅu lƒ©nh v·ª±c.
Machine Learning l√† m·ªôt nh√°nh quan tr·ªçng c·ªßa AI, cho ph√©p m√°y t√≠nh h·ªçc t·ª´ d·ªØ li·ªáu.
Deep Learning s·ª≠ d·ª•ng m·∫°ng neural nhi·ªÅu l·ªõp ƒë·ªÉ h·ªçc c√°c pattern ph·ª©c t·∫°p.
Natural Language Processing (NLP) gi√∫p m√°y t√≠nh hi·ªÉu v√† x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n.
Large Language Models nh∆∞ GPT, Claude, Gemini ƒëang c√°ch m·∫°ng h√≥a c√°ch ch√∫ng ta t∆∞∆°ng t√°c v·ªõi AI.
RAG (Retrieval Augmented Generation) k·∫øt h·ª£p vi·ªác t√¨m ki·∫øm th√¥ng tin v√† t·∫°o sinh c√¢u tr·∫£ l·ªùi.
Vector databases nh∆∞ ChromaDB, Pinecone, Weaviate l∆∞u tr·ªØ embeddings ƒë·ªÉ t√¨m ki·∫øm semantic.
""",
        
        "programming.txt": """
Python l√† m·ªôt ng√¥n ng·ªØ l·∫≠p tr√¨nh ph·ªï bi·∫øn v√† d·ªÖ h·ªçc, ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i trong AI/ML.
LangChain l√† m·ªôt framework m·∫°nh m·∫Ω ƒë·ªÉ x√¢y d·ª±ng ·ª©ng d·ª•ng AI v·ªõi LLMs.
LangGraph cho ph√©p x√¢y d·ª±ng c√°c workflow ph·ª©c t·∫°p v·ªõi AI agents v√† state management.
Vector databases nh∆∞ ChromaDB, Pinecone gi√∫p l∆∞u tr·ªØ v√† t√¨m ki·∫øm embeddings hi·ªáu qu·∫£.
Embedding models chuy·ªÉn ƒë·ªïi text th√†nh vector s·ªë ƒë·ªÉ so s√°nh ƒë·ªô t∆∞∆°ng ƒë·ªìng semantic.
Sentence Transformers l√† th∆∞ vi·ªán ph·ªï bi·∫øn ƒë·ªÉ t·∫°o embeddings ch·∫•t l∆∞·ª£ng cao.
Hugging Face cung c·∫•p h√†ng ng√†n pre-trained models cho NLP v√† computer vision.
"""
    }
    
    for filename, content in sample_docs.items():
        file_path = os.path.join(data_dir, filename)
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content.strip())
    
    print(f"Created {len(sample_docs)} sample documents in {data_dir}")
    
    # Return file paths for loading
    return [os.path.join(data_dir, filename) for filename in sample_docs.keys()]


if __name__ == "__main__":
    # Test web document loading
    urls = setup_web_documents()
    processor = DocumentProcessor(type('Config', (), {'embedding_model': 'all-MiniLM-L6-v2'})())
    
    # Try loading from web
    web_docs = processor.load_documents_from_urls(urls[:2])  # Test with first 2 URLs
    
    if web_docs:
        print(f"‚úÖ Successfully loaded {len(web_docs)} chunks from web")
        print(f"Sample content: {web_docs[0]['content'][:200]}...")
    else:
        print("‚ùå Web loading failed, falling back to sample documents")
        setup_sample_documents()